{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Factiva metadata\n",
    "This notebook will parse RTF files downloaded from Factiva in batches of up to 100 articles at a time, and create a single CSV file with all of the metadata and full-text from those articles. \n",
    "\n",
    "### Download RTF files from Factiva\n",
    "1. To begin, search for your content on Factiva. \n",
    "2. In the Factiva search results, select the checkbox next to \"Headlines\" to choose the first 100 articles from your results. (You can also select individual articles of interest, up to 100)\n",
    "\n",
    "![select all of the search results](fact_headlines.png)\n",
    "\n",
    "3. Choose the RTF button, and then \"Article Format\" to save those 100 articles into a single RTF file. (This works best if you create a new folder for all of the RTF files you are going to download. That folder should not contain any other content.)\n",
    "\n",
    "![article format option from RTF](fact_art_format.png)\n",
    "\n",
    "4. Select the \"Next 100\" link to view the next 100 search results in Factiva.\n",
    "5. Select \"Clear\" to unselect the previous list of 100, and repeat steps 2-3 to save the current pages of results for any articles you want to capture. \n",
    "\n",
    "### Convert RTF files to .txt\n",
    "Once you've downloaded all of the articles you want, go to the Shell (Terminal on Mac), and from the directory that contains your RTF downloads, run the following command to create a .txt file copy of each .rtf file:\n",
    "\n",
    "```\n",
    "textutil -convert txt *.rtf\n",
    "```\n",
    "\n",
    "You can also choose the path to your rtf downloads folder from your working directory. For example:\n",
    "\n",
    "```\n",
    "textutil -convert txt Desktop/Factiva/*.rtf\n",
    "```\n",
    "\n",
    "### Parse .txt files\n",
    "\n",
    "Now you should have a single folder with both .rtf and .txt file copies of the same. The next section includes Python code that will parse all of those .txt files into a single .csv file that includes the complete metadata and full-text from t articles. Each row of the .csv will represent a single article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: import Python libraries required to run code below\n",
    "import re\n",
    "import csv\n",
    "import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sep is a variable we will use to separate each article. \n",
    "#In the .rtf downloads there is a hexadecimal character 0x0C between each article (which skips to the next page)) \n",
    "# we can represent that character here with the escape character \\f\n",
    "sep = '\\f'\n",
    "\n",
    "#choose which fields you would like to work with (see the full list below)\n",
    "#fieldnames = ['SE', 'HD', 'WC', 'PD', 'SN', 'SC', 'LA', 'CY', 'LP', 'CO', 'TD', 'NS', 'RE', 'PUB', 'AN']\n",
    "fieldnames = ['SE', 'HD', 'WC', 'PD', 'SN', 'SC', 'LA', 'CY', 'NS', 'CO', 'RE', 'PUB', 'AN', 'LP', 'TD' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factiva field codes:\n",
    "\n",
    "- AN = Accession Number\n",
    "- ART = Captions, description of graphics\n",
    "- BY = Author\n",
    "- CLM = Column\n",
    "- CO = Dow Jones Ticker Symbol/Company Code Name\n",
    "- CR = Credit\n",
    "- CT = Contact \n",
    "- CX = Correction\n",
    "- DLN = Dateline\n",
    "- ED = Edition\n",
    "- HD = Headline\n",
    "- IN = Industry Code:Descriptor\n",
    "- LA = Language\n",
    "- LP = Lead Paragraph\n",
    "- NS = Subject Code:Descriptor\n",
    "- PG = Page\n",
    "- PUB = Publisher Name\n",
    "- RE = Region Code:Descriptor\n",
    "- RF = Reference\n",
    "- RST = Source Restrictor Code\n",
    "- SC = Source Code\n",
    "- SE = Section\n",
    "- SN = Source Name\n",
    "- TD = Text following lead paragraphs\n",
    "- VOL = Volume\n",
    "- WC = Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function processes the files and takes the path to a folder full of .txt files you want to process as its argument\n",
    "def factiva_to_csv(path):\n",
    "    #create a new csv\n",
    "    with open('factiva_metadata.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        \n",
    "        # write the fieldnames defined above as the headers of your csv \n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        #cycle through every text file in the path given in the function's argument\n",
    "        files_all = glob.iglob(path + \"*.txt\")\n",
    "        for k, filename in enumerate(files_all):\n",
    "\n",
    "            #remove the path, whitespace, and '.txt' from filename to later use when printing output\n",
    "            file_id = filename[:-4].strip(path)\n",
    "            \n",
    "            #open the files\n",
    "            with open(filename, 'r', encoding='utf-8') as in_file:\n",
    "                # text var for string of all docs\n",
    "                text = in_file.read()\n",
    "                \n",
    "                #remove the \"Search Summary\" from the end of each document\n",
    "                search_sum = '\\nSearch Summary\\n'\n",
    "                drop_search_sum = re.split(search_sum, text)\n",
    "                text = drop_search_sum[0]\n",
    "                \n",
    "                # split string by separator into single articles\n",
    "                docs = re.split(sep, text)\n",
    "\n",
    "                # loop through every doc to collect metadata and full text\n",
    "                for i, doc in enumerate(docs):\n",
    "\n",
    "                    # remove white space from beginning and end of each article\n",
    "                    doc = doc.strip()\n",
    "\n",
    "                    # skip any empty docs\n",
    "                    if doc==\"\":\n",
    "                        continue\n",
    "\n",
    "                    #create an empty dictionary that will later contain metadata keys (fieldnames) and the content for each metadata field\n",
    "                    metadata_dict = {}\n",
    "                    \n",
    "                    #this regular expression looks for the 2 or 3 character field codes in the document\n",
    "                    regex = '(\\s\\s\\s[A-Z]{2,3})'\n",
    "                    \n",
    "                    #split up each document based on the 2-3 char metadata field codes\n",
    "                    splits = re.split(regex, doc)\n",
    "\n",
    "                    #create variables to hold metadata and content\n",
    "                    key, value = '', ''\n",
    "                    \n",
    "                    #cycle through each metadata element\n",
    "                    for k, split in enumerate(splits):\n",
    "                       \n",
    "                        #check for the SE field, which doesn't follow the same syntax as other fields\n",
    "                        if re.match('^SE', split):\n",
    "                            key = 'SE'\n",
    "                            #print(\"SE\")\n",
    "                            value = split.strip('SE\\n')\n",
    "                        \n",
    "                        #if we match a 2-3 char code, assign it to key\n",
    "                        elif re.match(regex, split):\n",
    "                            key = split.strip()\n",
    "                            #print('key=', key)\n",
    "                        \n",
    "                        #if we don't match a 2-3 char code assign it to value\n",
    "                        else:\n",
    "                            value = split.strip()\n",
    "                            #print('value=',value)\n",
    "                        \n",
    "                        #only add keys and values to our dictionary if they match the fieldnames we chose above\n",
    "        \n",
    "                        if key in fieldnames:\n",
    "                            metadata_dict[key] = value\n",
    "                    \n",
    "                    #write each row to the csv containing all of the values that match existing fieldnames/keys\n",
    "                    writer.writerow(metadata_dict)\n",
    "                    \n",
    "                #output to let us know the .txt files that are being processed\n",
    "                print(\"Writing\", file_id)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the function by pointing to the path to a folder full of .txt files you want to process\n",
    "factiva_to_csv('rtf/') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: CSV files and full text fields\n",
    "Note that the full-text from Factiva is divided up into two fields: \n",
    "* LP = Lead Paragraph\n",
    "* TD = Text following lead paragraphs\n",
    "\n",
    "*Important:* At times character encoding anomalies will cause a full-text field to break up into new rows when importing CSV files into Microsoft Excel. You can import the CSV \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
