{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Factiva metadata\n",
    "This notebook will parse RTF files downloaded from Factiva in batches of up to 100 articles at a time, and create a a Pandas dataframe (which you can export as a CSV file) with all of the metadata and full-text from those articles. \n",
    "\n",
    "### Download RTF files from Factiva\n",
    "1. To begin, search for your content on Factiva. \n",
    "2. In the Factiva search results, select the checkbox next to \"Headlines\" to choose the first 100 articles from your results. (You can also select individual articles of interest, up to 100)\n",
    "\n",
    "![select all of the search results](fact_headlines.png)\n",
    "\n",
    "3. Choose the RTF button, and then \"Article Format\" to save those 100 articles into a single RTF file. (This works best if you create a new folder for all of the RTF files you are going to download. That folder should not contain any other content.)\n",
    "\n",
    "![article format option from RTF](fact_art_format.png)\n",
    "\n",
    "4. Select the \"Next 100\" link to view the next 100 search results in Factiva.\n",
    "5. Select \"Clear\" to unselect the previous list of 100, and repeat steps 2-3 to save the current pages of results for any articles you want to capture. \n",
    "\n",
    "### Convert RTF files to .txt (Mac)\n",
    "Once you've downloaded all of the articles you want, open Terminal and from the directory that contains your RTF downloads, run the following command to create a .txt file copy of each .rtf file:\n",
    "\n",
    "```\n",
    "textutil -convert txt *.rtf\n",
    "```\n",
    "\n",
    "You can also choose the path to your rtf downloads folder from your working directory. For example:\n",
    "\n",
    "```\n",
    "textutil -convert txt Desktop/Factiva/*.rtf\n",
    "```\n",
    "\n",
    "For Windows and other operating systems, you will want to find a utility to batch convert .rtf to .txt files. See [UnRtf](https://gnuwin32.sourceforge.net/packages/unrtf.htm) as one possibility.\n",
    "\n",
    "### Parse .txt files \n",
    "\n",
    "Now you should have a single folder with both .rtf and .txt file copies of the same. The next section includes Python code that will parse all of those .txt files into a single pandas dataframe that includes the complete metadata and full-text from the articles. Each row of the dataframe will represent a single article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: import Python libraries required to run code below\n",
    "import re\n",
    "import csv\n",
    "import glob \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sep is a variable we will use to separate each article. \n",
    "#In the .rtf downloads there is a hexadecimal character 0x0C between each article (which skips to the next page)) \n",
    "# we can represent that character here with the escape character \\f\n",
    "sep = '\\f'\n",
    "\n",
    "#choose which fields you would like to work with (see the full list below)\n",
    "#fieldnames = ['SE', 'HD', 'WC', 'PD', 'SN', 'SC', 'LA', 'CY', 'LP', 'CO', 'TD', 'NS', 'RE', 'PUB', 'AN']\n",
    "fieldnames = ['SE', 'HD', 'WC', 'PD', 'SN', 'SC', 'LA', 'CY', 'NS', 'CO', 'RE', 'PUB', 'AN', 'LP', 'TD' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factiva field codes:\n",
    "\n",
    "- AN = Accession Number\n",
    "- ART = Captions, description of graphics\n",
    "- BY = Author\n",
    "- CLM = Column\n",
    "- CO = Dow Jones Ticker Symbol/Company Code Name\n",
    "- CR = Credit\n",
    "- CT = Contact \n",
    "- CX = Correction\n",
    "- DLN = Dateline\n",
    "- ED = Edition\n",
    "- HD = Headline\n",
    "- IN = Industry Code:Descriptor\n",
    "- LA = Language\n",
    "- LP = Lead Paragraph\n",
    "- NS = Subject Code:Descriptor\n",
    "- PG = Page\n",
    "- PUB = Publisher Name\n",
    "- RE = Region Code:Descriptor\n",
    "- RF = Reference\n",
    "- RST = Source Restrictor Code\n",
    "- SC = Source Code\n",
    "- SE = Section\n",
    "- SN = Source Name\n",
    "- TD = Text following lead paragraphs\n",
    "- VOL = Volume\n",
    "- WC = Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function processes the files and takes the path to a folder full of .txt files you want to process as its argument\n",
    "def factiva_to_df(path):\n",
    "    \n",
    "    df = pd.DataFrame(columns = fieldnames)       \n",
    "        \n",
    "    #cycle through every text file in the path given in the function's argument\n",
    "    files_all = sorted(glob.iglob(path + \"*.txt\"))\n",
    "    for filename in files_all:\n",
    "\n",
    "        #remove the path, whitespace, and '.txt' from filename to later use when printing output\n",
    "        file_id = filename[:-4].strip(path)\n",
    "\n",
    "        #open the files\n",
    "        with open(filename, 'r', encoding='utf-8') as in_file:\n",
    "            # text var for string of all docs\n",
    "            text = in_file.read()\n",
    "            \n",
    "            #output to let us know the .txt files that are being processed\n",
    "            print(\"Loading\", file_id)    \n",
    "            \n",
    "            in_file.close()\n",
    "            \n",
    "        #remove the \"Search Summary\" from the end of each document\n",
    "        search_sum = '\\nSearch Summary\\n'\n",
    "        drop_search_sum = re.split(search_sum, text)\n",
    "        text = drop_search_sum[0]\n",
    "\n",
    "        # split string by separator into single articles\n",
    "        docs = re.split(sep, text)\n",
    "\n",
    "        # loop through every doc to collect metadata and full text\n",
    "        for doc in docs:\n",
    "\n",
    "            # remove white space from beginning and end of each article\n",
    "            doc = doc.strip()\n",
    "\n",
    "            # skip any empty docs\n",
    "            if doc==\"\":\n",
    "                continue\n",
    "\n",
    "            #create an empty dictionary that will later contain metadata keys (fieldnames) and the content for each metadata field\n",
    "            metadata_dict = {}\n",
    "\n",
    "            #this regular expression looks for the 2 or 3 character field codes in the document\n",
    "            regex = '(\\s[A-Z]{2,3}\\n)'\n",
    "\n",
    "            #split up each document based on the 2-3 char metadata field codes\n",
    "            splits = re.split(regex, doc)\n",
    "\n",
    "            #create variables to hold metadata and content\n",
    "            key, value = '', ''\n",
    "\n",
    "            #cycle through each metadata element\n",
    "            for split in splits:\n",
    "\n",
    "                #check for the SE and HD fields, which sometimes don't follow the same syntax as other fields\n",
    "                if re.match('^HD\\s', split):\n",
    "                    key = 'HD'\n",
    "                    value = split[3:].strip()\n",
    "\n",
    "                elif re.match('^SE\\s', split):\n",
    "                    key = 'SE'\n",
    "                    value = split[3:].strip()\n",
    "\n",
    "                #if we match a 2-3 char code, assign it to key\n",
    "                elif re.match(regex, split):\n",
    "                    key = split.strip()\n",
    "\n",
    "                #if we don't match a 2-3 char code assign it to value\n",
    "                else:\n",
    "                    value = split.strip()\n",
    "                    #print('value=',value)\n",
    "\n",
    "                #only add keys and values to our dictionary if they match the fieldnames we chose above\n",
    "\n",
    "                if key in fieldnames:\n",
    "                    metadata_dict[key] = value\n",
    "\n",
    "            #write each row to the csv containing all of the values that match existing fieldnames/keys\n",
    "            df.loc[len(df)] = metadata_dict\n",
    "        \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the function by pointing to the path to a folder full of .txt files you want to process\n",
    "df = factiva_to_df('fact_rtfs/') \n",
    "print('\\nNumber of articles processed:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data can be cleaned or examined as a dataframe here\n",
    "\n",
    "#uncomment to create a new full text column combining the LP (lead paragraph) and TD (Text following lead paragraphs) columns\n",
    "#df['FT'] = df['LP'] + '\\n\\n' + df['TD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to a csv file\n",
    "df.to_csv('factiva_metadata.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: CSV files and full text fields\n",
    "Note that the full-text from Factiva is divided up into two fields: \n",
    "* LP = Lead Paragraph\n",
    "* TD = Text following lead paragraphs\n",
    "\n",
    "### Character encoding issues in Excel\n",
    "At times character encoding anomalies will cause a full-text field to break up into new rows when importing CSV files into Microsoft Excel. Import the CSV files into Google Sheets or other spreadsheet applications for better results. Here are directions:\n",
    "1. Create a new blank Google Sheet and choose File > Import. \n",
    "2. Select the CSV file that you exported and in the pop up change the Separator type to \"Comma\" and uncheck the \"Convert text...\" option. The data should be properly parsed in Google Sheets. \n",
    "![Pop-up box offering how to import the CSV file](https://raw.githubusercontent.com/chennesy/pq_parser/49d2a4e99c2a43ee6799297fcef4cc3e31b10900//imgs/import_file.png \"Import file into Google Sheets\")\n",
    "3. If you prefer to work with the data in Excel, then go to File > Download as > Microsoft Excel and save a copy to your computer.\n",
    "4. When opening the file you may get an alert from Excel. If so, choose \"Yes\" to recover data.\n",
    "\n",
    "![Excel alert warning there may be problems with the metadata](https://raw.githubusercontent.com/chennesy/pq_parser/49d2a4e99c2a43ee6799297fcef4cc3e31b10900//imgs/alert.png \"Excel alert\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Cody Hennesy and David Naughton (University of Minnesota, Twin Cities, Libraries). Feel free to email Cody (chennesy@umn.edu) with any questions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
